<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <meta name="description"
        content="ViCor: Bridging Visual Understanding and Commonsense Reasoning with Large Language Models.">
  <meta name="keywords" content="Multimodal Generation, Interleaved Vision-and-Language Generation">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <title>ViCor: Bridging Visual Understanding and Commonsense Reasoning with Large Language Models.</title>

  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=G-PYVRSFMDRL"></script>
  <script>
    window.dataLayer = window.dataLayer || [];

    function gtag() {
      dataLayer.push(arguments);
    }

    gtag('js', new Date());

    gtag('config', 'G-PYVRSFMDRL');
  </script>

  <link href="https://fonts.googleapis.com/css?family=Google+Sans|Noto+Sans|Castoro"
  rel="stylesheet">

  <link rel="stylesheet" href="./static/css/bulma.min.css">
  <link rel="stylesheet" href="./static/css/bulma-carousel.min.css">
  <link rel="stylesheet" href="./static/css/bulma-slider.min.css">
  <link rel="stylesheet" href="./static/css/fontawesome.all.min.css">
  <link rel="stylesheet"
  href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">
  <link rel="icon" href="./static/images/icon.png">
  <link rel="stylesheet" href="./static/css/index.css">

  <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
  <script defer src="./static/js/fontawesome.all.min.js"></script>
  <script src="./static/js/bulma-carousel.min.js"></script>
  <script src="./static/js/bulma-slider.min.js"></script>
  <script src="./static/js/index.js"></script>
  <title>Title Size Change</title>
    <style>
        .publication-title {
            font-size: 24px; /* Change to desired font size */
        }
        .title.is-1 {
            font-size: 36px; /* Change to desired font size */
        }
        .title.is-2 {
            font-size: 30px; /* Change to desired font size */
        }
    </style>
</head>
<body>


<section class="hero">
  <div class="hero-body">
    <div class="container is-max-desktop">
      <div class="columns is-centered">
        <div class="column has-text-centered">
          <h2 class="title is-1 publication-title">ViCor: Bridging Visual Understanding and Commonsense Reasoning with Large Language Models</h2>
          <div class="is-size-5 publication-authors">
            <span class="author-block">
                <a href="https://kevinz-01.github.io/" style="color:#f68946;font-weight:normal;">Kaiwen Zhou</a>,                
            </span>
            <span class="author-block">
              <a href="https://scholar.google.com/citations?hl=en&user=C6Wu8M0AAAAJ&view_op=list_works" style="color:#F2A900;font-weight:normal;">Kwonjoon Lee</a>,
            </span>
            <span class="author-block">
              <a href="https://scholar.google.com/citations?user=4LAT5WYAAAAJ&hl=en" style="color:#F2A900;font-weight:normal;">Teruhisa Misu</a>,
            </span>
            <span class="author-block">
              <a href="https://eric-xw.github.io/" style="color:#f68946;font-weight:normal;">Xin Eric Wang</a>,
            </span>  
          </div>
          <div class="is-size-5 publication-authors">
            <span class="author-block"><b style="color:#f68946; font-weight:normal">&#x25B6 </b> University of California, Santa Cruz; </b></span>
            <span class="author-block"><b style="color:#F2A900; font-weight:normal">&#x25B6 </b> Honda Research Institute; </span>
          </div>

          <div class="column has-text-centered">
            <div class="publication-links">
              <!-- PDF Link. -->
              <span class="link-block">
                <a href="https://arxiv.org/pdf/2310.05872.pdf"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="fas fa-file-pdf"></i>
                  </span>
                  <span>Paper</span>
                </a>
              </span>
              <span class="link-block">
                <a href="https://arxiv.org/abs/2310.05872"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="ai ai-arxiv"></i>
                  </span>
                  <span>arXiv</span>
                </a>
              </span>
              <!-- Code Link. -->
              <span class="link-block">
                <a href="https://github.com/eric-ai-lab/ViCor"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="fab fa-github"></i>
                  </span>
                  <span>Code</span>
                  </a>
              </span>
            </div>

          </div>
        </div>
      </div>
    </div>
  </div>
</section>


<section class="section">
  <div class="container is-max-desktop">
    <!-- Abstract. -->
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">Abstract</h2>
        <div class="content has-text-justified">
          <p>
            In our work, we explore the synergistic capabilities of pre-trained vision-and-language models (VLMs) and large language models
            (LLMs) for visual commonsense reasoning (VCR). We categorize the problem of VCR into visual commonsense understanding (VCU) and
            visual commonsense inference (VCI). For VCU, which involves perceiving the literal visual content, pre-trained VLMs exhibit
            strong cross-dataset generalization. On the other hand, in VCI, where the goal is to infer conclusions beyond image content, 
            VLMs face difficulties. We find that a baseline where VLMs provide perception results (image captions) to LLMs leads to improved
            performance on VCI. However, we identify a challenge with VLMs' passive perception, which often misses crucial context 
            information, leading to incorrect or uncertain reasoning by LLMs. To mitigate this issue, we suggest a collaborative approach
            where LLMs, when uncertain about their reasoning, actively direct VLMs to concentrate on and gather relevant visual elements to 
            support potential commonsense inferences. In our method, named ViCor, pre-trained LLMs serve as problem classifiers to analyze
            the problem category, VLM commanders to leverage VLMs differently based on the problem classification, and visual commonsense 
            reasoners to answer the question. VLMs will perform visual recognition and understanding. We evaluate our framework on two VCR 
            benchmark datasets and outperform all other methods that do not require in-domain supervised fine-tuning.
          </p>
        </div>
      </div>
    </div>
  </div>
</section>

<section class="hero teaser">
  <div class="container is-max-desktop">
    <div class="hero-body">
      <img id="teaser" width="150%" src="./static/images/fig1.png">
      <h2 class="subtitle has-text-centered">
        <p style="font-family:Times New Roman"><b>Figure 1. In the real world, different visual commonsense questions require different levels of reasoning and visual understanding. 
          Therefore, the model should be able to reason what kind of visual understanding needs to be performed to answer the visual question. </b></p>
      </h2>
    </div>
  </div>
</section>

<section class="section">
  <div class="container is-max-desktop">
    <div class="columns is-centered has-text-centered">
      <div class="column is-five-fifths">
        <h2 class="title is-3"><img id="painting_icon" width="5%" src="https://cdn-icons-png.flaticon.com/512/5379/5379860.png"> Active Visual Understanding and Commonsense Reasoning with LLMs </h2> 
      </div>
    </div>

        <div class="columns is-centered has-text-centered">
          <div class="column is-six-fifths">
            <div class="content has-text-justified">
              <ul>
                <li>We design a framework where LVLMs serve for visual understanding, and LLMs serve for commonsense reasoning. </li>
                <li>LLMs can call VLMs in different ways by analyzing the question type and the initial answer confidence.</li>
                <li>LLMs can reason based on the question context and guide the VLMs to extract crucial visual information from the image.</li>
              </ul>
            </div>        
            <img id="model" width="100%" src="./static/images/fig2.png">
            <h3 class="subtitle has-text-centered">
              <p style="font-family:Times New Roman"><b>Figure 2. ViCor pipeline.</b></p>
            </h3>   


        </div>
  </div>
</section>

<section class="section">
  <div class="container is-max-desktop">
    <div class="columns is-centered has-text-centered">
      <div class="column is-five-fifths">
        <h2 class="title is-3"><img id="painting_icon" width="5%" src="https://cdn-icons-png.flaticon.com/512/5379/5379860.png"> Qualitative Examples</h2> 
      </div>
    </div>
        <div class="columns is-centered has-text-centered">
          <div class="column is-six-fifths">
            <div class="content has-text-justified">
              <p>
                Qualitative examples from <b>ViCor</b>. 
              </p>
            </div>        
            <img id="model" width="100%" src="./static/images/fig3.png">
            <h3 class="subtitle has-text-centered">
              <p style="font-family:Times New Roman"><b>Figure 3.
                **Left**: The ITA called by the LLM corrects the initial unconfident reasoning. 
                **Middle, Right**: The LLM corrects the initial reasoning after giving the observation of the visual factor from BLIP2. </b></p>
            </h3>   
        </div>
  </div>
</section>

<section class="section" id="BibTeX">
  <div class="container is-max-desktop content">
    <h2 class="title">BibTeX</h2>
    <pre><code>@article{zhou2023vicor,
  title={Vicor: Bridging visual understanding and commonsense reasoning with large language models},
  author={Zhou, Kaiwen and Lee, Kwonjoon and Misu, Teruhisa and Wang, Xin Eric},
  journal={arXiv preprint arXiv:2310.05872},
  year={2023}
}
    </code></pre>
  </div>
</section>


<footer class="footer">
  <div class="container">
    <div class="columns is-centered">
      <div class="column is-8">
        <div class="content">
          <p>
            This website is adapted from <a rel="license"
            href="https://github.com/nerfies/nerfies.github.io">Nerfies</a> and <a rel="license"
            href="https://gligen.github.io/">GLIGEN</a>, licensed under a Creative Commons Attribution-ShareAlike 4.0 International License.
            This website is licensed under a <a rel="license"
                                                href="http://creativecommons.org/licenses/by-sa/4.0/">Creative
            Commons Attribution-ShareAlike 4.0 International License</a>.
          </p>
        </div>
      </div>
    </div>
  </div>
</footer>

</body>
</html>
